{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pickled data\n",
    "import pickle\n",
    "cleaned_data = pd.read_pickle(r\"C:\\Users\\maorb\\Classes\\Classical_ML\\YDATA-kaggle-assignment\\data\\processed\\cleaned_data_Maor.pkl\")\n",
    "X_train = pd.read_pickle(r\"C:\\Users\\maorb\\Classes\\Classical_ML\\YDATA-kaggle-assignment\\data\\processed\\X_train.pkl\")\n",
    "X_test = pd.read_pickle(r\"C:\\Users\\maorb\\Classes\\Classical_ML\\YDATA-kaggle-assignment\\data\\processed\\X_test.pkl\")\n",
    "y_train = pd.read_pickle(r\"C:\\Users\\maorb\\Classes\\Classical_ML\\YDATA-kaggle-assignment\\data\\processed\\y_train.pkl\")\n",
    "y_test = pd.read_pickle(r\"C:\\Users\\maorb\\Classes\\Classical_ML\\YDATA-kaggle-assignment\\data\\processed\\y_test.pkl\")\n",
    "predictions = pd.read_csv(r'C:\\Users\\maorb\\Classes\\Classical_ML\\YDATA-kaggle-assignment\\predictions\\predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r\"C:\\Users\\maorb\\Classes\\Classical_ML\\YDATA-kaggle-assignment\\data\\raw\\train_dataset_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365798, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19510, 15)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find how many duplicated rows we have\n",
    "duplicates = raw_data[raw_data.duplicated()]\n",
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 365798 entries, 0 to 370631\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   session_id              365107 non-null  float64       \n",
      " 1   DateTime                365073 non-null  datetime64[ns]\n",
      " 2   user_id                 365798 non-null  float64       \n",
      " 3   product                 365736 non-null  category      \n",
      " 4   campaign_id             365072 non-null  category      \n",
      " 5   webpage_id              365070 non-null  category      \n",
      " 6   user_group_id           350779 non-null  category      \n",
      " 7   gender                  351429 non-null  category      \n",
      " 8   age_level               351436 non-null  float64       \n",
      " 9   user_depth              350771 non-null  float64       \n",
      " 10  city_development_index  266504 non-null  float64       \n",
      " 11  var_1                   365737 non-null  float64       \n",
      " 12  is_click                365798 non-null  float64       \n",
      " 13  product_category        365167 non-null  category      \n",
      "dtypes: category(6), datetime64[ns](1), float64(7)\n",
      "memory usage: 27.2 MB\n"
     ]
    }
   ],
   "source": [
    "cleaned_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month\n",
      "Day\n",
      "Hour\n",
      "Minute\n",
      "weekday\n",
      "product\n",
      "campaign_id\n",
      "webpage_id\n",
      "gender\n",
      "age_level\n",
      "user_depth\n",
      "city_development_index\n",
      "var_1\n",
      "product_category\n",
      "Month\n",
      "Day\n",
      "Hour\n",
      "Minute\n",
      "weekday\n",
      "product\n",
      "campaign_id\n",
      "webpage_id\n",
      "gender\n",
      "age_level\n",
      "user_depth\n",
      "city_development_index\n",
      "var_1\n",
      "product_category\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def feature_generation(df):\n",
    "    \"\"\"Generate date/time features and fill missing values in a faster, \n",
    "       more scalable way without repeated group-based ffill/bfill.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # If not already a datetime, convert:\n",
    "    # df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    \n",
    "    # -- 1) Create date/time features --\n",
    "    df['Month'] = df['DateTime'].dt.month\n",
    "    df['Day'] = df['DateTime'].dt.day\n",
    "    df['Hour'] = df['DateTime'].dt.hour\n",
    "    df['Minute'] = df['DateTime'].dt.minute\n",
    "    df['weekday'] = df['DateTime'].dt.weekday\n",
    "    \n",
    "    # -- 2) Drop unnecessary columns --\n",
    "    df.drop(columns=['DateTime', 'session_id'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # -- 3) Make user_id a consistent type --\n",
    "    # (Strings are often safer keys for merges.)\n",
    "    df['user_id'] = df['user_id'].astype(str)\n",
    "    \n",
    "    # -- 4) Identify columns to fill by median vs. mode --\n",
    "    #    (You can tune these lists as needed.)\n",
    "    columns_to_fill_median = ['Month', 'Day', 'Hour', 'Minute', 'weekday']\n",
    "    columns_to_fill_mode = [\n",
    "        'product', 'campaign_id', 'webpage_id', 'gender', \n",
    "        'age_level', 'user_depth', 'city_development_index', \n",
    "        'var_1', 'product_category'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that actually exist in df\n",
    "    columns_to_fill_median = [c for c in columns_to_fill_median if c in df.columns]\n",
    "    columns_to_fill_mode = [c for c in columns_to_fill_mode if c in df.columns]\n",
    "    \n",
    "    # -- 5) Precompute the user-level medians/modes in one pass each --\n",
    "    if columns_to_fill_median:\n",
    "        median_df = (\n",
    "            df.groupby('user_id')[columns_to_fill_median]\n",
    "            .median()\n",
    "            .reset_index()\n",
    "        )\n",
    "    \n",
    "    # Mode can be tricky (pandas mode can return multiple values).\n",
    "    # We'll define a custom aggregator that picks the first mode if multiple modes exist.\n",
    "    def agg_mode(s):\n",
    "        m = s.mode(dropna=True)\n",
    "        return m.iloc[0] if len(m) > 0 else np.nan\n",
    "        \n",
    "    if columns_to_fill_mode:\n",
    "        mode_df = (\n",
    "            df.groupby('user_id')[columns_to_fill_mode]\n",
    "            .agg(agg_mode)\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "    # -- 6) Merge those statistics back to df --\n",
    "    # This is usually much more performant than repeated group transforms:\n",
    "    if columns_to_fill_median:\n",
    "        df = df.merge(\n",
    "            median_df, \n",
    "            on='user_id', \n",
    "            suffixes=('', '_median')\n",
    "        )\n",
    "    if columns_to_fill_mode:\n",
    "        df = df.merge(\n",
    "            mode_df, \n",
    "            on='user_id', \n",
    "            suffixes=('', '_mode')\n",
    "        )\n",
    "        \n",
    "    # -- 7) Fill missing values in df using the merged median/mode --\n",
    "    if columns_to_fill_median:\n",
    "        for col in columns_to_fill_median:\n",
    "            df[col] = df[col].fillna(df[f'{col}_median'])\n",
    "            df.drop(columns=[f'{col}_median'], inplace=True, errors='ignore')\n",
    "            \n",
    "    if columns_to_fill_mode:\n",
    "        for col in columns_to_fill_mode:\n",
    "            df[col] = df[col].fillna(df[f'{col}_mode'])\n",
    "            df.drop(columns=[f'{col}_mode'], inplace=True, errors='ignore')\n",
    "    \n",
    "    for col in columns_to_fill_median:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    for col in columns_to_fill_mode:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    df.drop(columns=['user_id','user_group_id'], inplace=True, errors='ignore')\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "X_train_u = feature_generation(X_train)\n",
    "X_test_u  = feature_generation(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_mode(df: pd.DataFrame, columns: list):\n",
    "\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            mode_value = df[column].mode()[0]  # Calculate the mode\n",
    "            df[column] = df[column].fillna(mode_value)  # Fill missing values with the mode\n",
    "    return df\n",
    "\n",
    "def fill_missing_with_median(df: pd.DataFrame, columns: list):\n",
    "\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            median_value = df[column].median()  # Calculate the median\n",
    "            df[column] = df[column].fillna(median_value)  # Fill missing values with the median\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product', 'campaign_id', 'webpage_id', 'gender', 'age_level',\n",
       "       'user_depth', 'city_development_index', 'var_1', 'product_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.copy()\n",
    "X_test2 = X_test.copy()\n",
    "X_train2['Month'] = X_train2['DateTime'].dt.month\n",
    "X_train2['Day'] = X_train2['DateTime'].dt.day\n",
    "X_train2['Hour'] = X_train2['DateTime'].dt.hour\n",
    "X_train2['Minute'] = X_train2['DateTime'].dt.minute\n",
    "X_train2['weekday'] = X_train2['DateTime'].dt.weekday\n",
    "\n",
    "X_test2['Month'] = X_test2['DateTime'].dt.month\n",
    "X_test2['Day'] = X_test2['DateTime'].dt.day\n",
    "X_test2['Hour'] = X_test2['DateTime'].dt.hour\n",
    "X_test2['Minute'] = X_test2['DateTime'].dt.minute\n",
    "X_test2['weekday'] = X_test2['DateTime'].dt.weekday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill each Na with X_trai\n",
    "columns_to_fill = [\"product\", \"campaign_id\", \"webpage_id\", \"user_group_id\", \"gender\", \"age_level\", \"user_depth\", \"city_development_index\", \"var_1\", \"product_category\",\n",
    "                     \"month\", \"day\", \"hour\"]\n",
    "columns_to_fill_median = [\"Month\", \"Day\", \"Hour\", \"Minute\", \"weekday\"]\n",
    "\n",
    "X_train_filled = fill_missing_with_mode(X_train2, columns_to_fill)\n",
    "X_test_filled = fill_missing_with_mode(X_test2, columns_to_fill)\n",
    "X_train_filled = fill_missing_with_median(X_train_filled, columns_to_fill_median)\n",
    "X_test_filled = fill_missing_with_median(X_test_filled, columns_to_fill_median)\n",
    "X_train_filled.drop(columns=['user_id','DateTime','session_id','user_group_id'], inplace=True)\n",
    "X_test_filled.drop(columns=['user_id','DateTime','session_id','user_group_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_filled.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_d = [\"product\", \"campaign_id\", \"webpage_id\", \"gender\", \"product_category\"]\n",
    "\n",
    "X_train_filled_d = pd.get_dummies(X_train_filled, columns = columns_to_d)\n",
    "X_test_filled_d = pd.get_dummies(X_test_filled, columns = columns_to_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03940886699507389"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42,class_weight='balanced')\n",
    "rf.fit(X_train_filled_d, y_train)\n",
    "y_pred = rf.predict(X_test_filled_d)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Apply SMOTE to balance the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_filled_d, y_train)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred2 = rf.predict(X_test_filled_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14379888268156424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "#adjuct the balance of the classes\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42, solver='liblinear',\n",
    "                           penalty='l2', C=0.1)\n",
    "model.fit(X_train_filled_d, y_train)\n",
    "\n",
    "y_pred_LR = model.predict(X_test_filled_d)\n",
    "print(f1_score(y_test, y_pred_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.14559499941030782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.61      0.74     68166\n",
      "         1.0       0.09      0.49      0.15      4994\n",
      "\n",
      "    accuracy                           0.60     73160\n",
      "   macro avg       0.51      0.55      0.44     73160\n",
      "weighted avg       0.88      0.60      0.70     73160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "model = ComplementNB()\n",
    "model.fit(X_train_filled_d, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_filled_d)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline? :()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9323260378678943"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data['is_click'].value_counts()[0]/cleaned_data['is_click'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1- cleaned_data['is_click'].value_counts()[0]/cleaned_data['is_click'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 292638 entries, 196 to 123594\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   session_id              292076 non-null  float64       \n",
      " 1   DateTime                292049 non-null  datetime64[ns]\n",
      " 2   user_id                 292638 non-null  float64       \n",
      " 3   product                 292584 non-null  category      \n",
      " 4   campaign_id             292063 non-null  category      \n",
      " 5   webpage_id              292048 non-null  category      \n",
      " 6   user_group_id           280649 non-null  category      \n",
      " 7   gender                  281173 non-null  category      \n",
      " 8   age_level               281178 non-null  float64       \n",
      " 9   user_depth              280646 non-null  float64       \n",
      " 10  city_development_index  213253 non-null  float64       \n",
      " 11  var_1                   292591 non-null  float64       \n",
      " 12  product_category        292139 non-null  category      \n",
      " 13  hour                    292049 non-null  float64       \n",
      " 14  day                     292049 non-null  float64       \n",
      " 15  month                   292049 non-null  float64       \n",
      "dtypes: category(6), datetime64[ns](1), float64(9)\n",
      "memory usage: 26.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['hour'] = X_train['DateTime'].dt.hour\n",
    "X_train['day'] = X_train['DateTime'].dt.day\n",
    "X_train['month'] = X_train['DateTime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train.drop(columns=['session_id', 'DateTime', 'user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "def objective(trial):\n",
    "    # --- 1) Suggest hyperparams ---\n",
    "    # Example search space: Feel free to expand or tune ranges\n",
    "    params = {\n",
    "        \"iterations\": 1000,\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 8),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 100),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "        # Commonly used knobs; random_strength, subsample, etc. can also be tuned\n",
    "        \"eval_metric\": \"F1\",\n",
    "        \"random_seed\": 42,\n",
    "        \"auto_class_weights\": \"Balanced\",\n",
    "        \"verbose\": 0  # Keep CatBoost silent\n",
    "    }\n",
    "\n",
    "    # --- 2) Create CatBoost model ---\n",
    "    model = CatBoostClassifier(**params)\n",
    "\n",
    "    # --- 3) Train/Validation split ---\n",
    "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
    "        X_train1,\n",
    "        y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train\n",
    "    )\n",
    "\n",
    "    # --- 4) Train ---\n",
    "    model.fit(\n",
    "        X_train_sub,\n",
    "        y_train_sub,\n",
    "        cat_features=cat_features,\n",
    "        eval_set=(X_val_sub, y_val_sub),\n",
    "        early_stopping_rounds=50,\n",
    "        use_best_model=True\n",
    "    )\n",
    "\n",
    "    # --- 5) Predict ---\n",
    "    y_pred_val = model.predict(X_val_sub)\n",
    "\n",
    "    # --- 6) Evaluate ---\n",
    "    f1 = f1_score(y_val_sub, y_pred_val)\n",
    "\n",
    "    # Return the F1 (Optuna will try to maximize this)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-23 21:27:34,790] A new study created in memory with name: no-name-8ce8d20c-b456-437c-8c24-278df2096bb7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdc95c3af0e49d083c41c6c90b7cb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-01-23 21:27:52,779] Trial 0 finished with value: 0.14241381636645753 and parameters: {'iterations': 700, 'depth': 7, 'learning_rate': 0.09744541619010162, 'l2_leaf_reg': 4.6845889641307, 'bagging_temperature': 0.6083577649428933}. Best is trial 0 with value: 0.14241381636645753.\n",
      "[I 2025-01-23 21:29:00,012] Trial 1 finished with value: 0.14715379312846932 and parameters: {'iterations': 900, 'depth': 8, 'learning_rate': 0.06056087732149749, 'l2_leaf_reg': 44.82689039760325, 'bagging_temperature': 0.3545388999625765}. Best is trial 1 with value: 0.14715379312846932.\n",
      "[I 2025-01-23 21:29:18,804] Trial 2 finished with value: 0.14246593421096043 and parameters: {'iterations': 400, 'depth': 6, 'learning_rate': 0.012942977089435637, 'l2_leaf_reg': 23.56083041913574, 'bagging_temperature': 0.4558729858095575}. Best is trial 1 with value: 0.14715379312846932.\n",
      "[I 2025-01-23 21:29:34,801] Trial 3 finished with value: 0.14265668849391955 and parameters: {'iterations': 700, 'depth': 7, 'learning_rate': 0.080796438459713, 'l2_leaf_reg': 92.0943432488394, 'bagging_temperature': 0.475600614080402}. Best is trial 1 with value: 0.14715379312846932.\n",
      "[I 2025-01-23 21:31:07,688] Trial 4 finished with value: 0.1476639519931419 and parameters: {'iterations': 1000, 'depth': 6, 'learning_rate': 0.09878531291168284, 'l2_leaf_reg': 75.88220809988779, 'bagging_temperature': 0.5426526136974688}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:31:14,758] Trial 5 finished with value: 0.14193717106338466 and parameters: {'iterations': 100, 'depth': 5, 'learning_rate': 0.07919274800341754, 'l2_leaf_reg': 21.655654371660916, 'bagging_temperature': 0.2731348218643188}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:31:38,377] Trial 6 finished with value: 0.14265668849391955 and parameters: {'iterations': 700, 'depth': 7, 'learning_rate': 0.07313717065119897, 'l2_leaf_reg': 43.54707008765233, 'bagging_temperature': 0.3064446390337354}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:32:05,187] Trial 7 finished with value: 0.1426403641881639 and parameters: {'iterations': 400, 'depth': 8, 'learning_rate': 0.031049701486487724, 'l2_leaf_reg': 96.73421630866153, 'bagging_temperature': 0.05288639536409112}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:32:19,125] Trial 8 finished with value: 0.14228179057176812 and parameters: {'iterations': 400, 'depth': 4, 'learning_rate': 0.05740633926082669, 'l2_leaf_reg': 16.671189982269283, 'bagging_temperature': 0.5555896608966858}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:32:45,839] Trial 9 finished with value: 0.14265743803061817 and parameters: {'iterations': 400, 'depth': 7, 'learning_rate': 0.0027632081531127053, 'l2_leaf_reg': 30.97229508667193, 'bagging_temperature': 0.6683993695585385}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:33:05,554] Trial 10 finished with value: 0.14198398431116196 and parameters: {'iterations': 1000, 'depth': 5, 'learning_rate': 0.03761110219796429, 'l2_leaf_reg': 72.55075722853606, 'bagging_temperature': 0.9453440034853672}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:33:33,086] Trial 11 finished with value: 0.1421941711527752 and parameters: {'iterations': 1000, 'depth': 8, 'learning_rate': 0.0929529962862502, 'l2_leaf_reg': 64.33952272843467, 'bagging_temperature': 0.7830590926048931}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:33:51,274] Trial 12 finished with value: 0.1433358407987417 and parameters: {'iterations': 900, 'depth': 6, 'learning_rate': 0.05891503727110363, 'l2_leaf_reg': 60.757133260029896, 'bagging_temperature': 0.271256291978086}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:34:04,181] Trial 13 finished with value: 0.14195573282767188 and parameters: {'iterations': 800, 'depth': 5, 'learning_rate': 0.04522327290581978, 'l2_leaf_reg': 76.57520207708575, 'bagging_temperature': 0.09596522705782784}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:36:43,461] Trial 14 finished with value: 0.14685642226625834 and parameters: {'iterations': 900, 'depth': 8, 'learning_rate': 0.06732309624113147, 'l2_leaf_reg': 45.250976255460664, 'bagging_temperature': 0.32834021621388704}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:37:15,302] Trial 15 finished with value: 0.14398544131028207 and parameters: {'iterations': 1000, 'depth': 6, 'learning_rate': 0.08612888043149991, 'l2_leaf_reg': 82.70889319860106, 'bagging_temperature': 0.7856572385711114}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:37:32,643] Trial 16 finished with value: 0.14198398431116196 and parameters: {'iterations': 600, 'depth': 4, 'learning_rate': 0.02301173283485969, 'l2_leaf_reg': 54.95664099116852, 'bagging_temperature': 0.16643462522036656}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:37:50,223] Trial 17 finished with value: 0.14205827528696163 and parameters: {'iterations': 800, 'depth': 5, 'learning_rate': 0.06475781964812169, 'l2_leaf_reg': 37.13556167482765, 'bagging_temperature': 0.4307283433006669}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:39:04,031] Trial 18 finished with value: 0.1462921510578381 and parameters: {'iterations': 900, 'depth': 6, 'learning_rate': 0.09722642044234332, 'l2_leaf_reg': 67.10077386399205, 'bagging_temperature': 0.7275165541861098}. Best is trial 4 with value: 0.1476639519931419.\n",
      "[I 2025-01-23 21:39:12,640] Trial 19 finished with value: 0.14193717106338466 and parameters: {'iterations': 100, 'depth': 7, 'learning_rate': 0.038977556807282585, 'l2_leaf_reg': 52.49479356567382, 'bagging_temperature': 0.39635227325266853}. Best is trial 4 with value: 0.1476639519931419.\n"
     ]
    }
   ],
   "source": [
    "# Create study that aims to maximize F1\n",
    "import optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Optimize over 'objective' for a certain number of trials\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['month'] = X_train['DateTime'].dt.month\n",
    "X_train['day'] = X_train['DateTime'].dt.day\n",
    "X_train['hour'] = X_train['DateTime'].dt.hour\n",
    "X_train['minute'] = X_train['DateTime'].dt.minute\n",
    "X_train['weekday'] = X_train['DateTime'].dt.weekday\n",
    "\n",
    "X_test['month'] = X_test['DateTime'].dt.month\n",
    "X_test['day'] = X_test['DateTime'].dt.day\n",
    "X_test['hour'] = X_test['DateTime'].dt.hour\n",
    "X_test['minute'] = X_test['DateTime'].dt.minute\n",
    "X_test['weekday'] = X_test['DateTime'].dt.weekday\n",
    "\n",
    "X_train.drop(columns=['DateTime', 'session_id', 'user_id', 'user_group_id'], inplace=True)\n",
    "X_test.drop(columns=['DateTime', 'session_id', 'user_id', 'user_group_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Categorical features: ['product', 'campaign_id', 'webpage_id', 'gender', 'product_category']\n",
      "INFO:__main__:Categorical features: ['product', 'campaign_id', 'webpage_id', 'gender', 'product_category']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['product', 'campaign_id', 'webpage_id', 'gender', 'product_category']"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# Define the ModelTrainer class\n",
    "class ModelTrainer:\n",
    "    def __init__(self, data_dir: False, model_name: str = \"catboost\", cat_features: list = None):\n",
    "        #self.data_dir = Path(data_dir)\n",
    "        self.model_name = model_name\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_data(self):\n",
    "        self.logger.info(f\"Loading preprocessed data from {self.data_dir}...\")\n",
    "        X_train = pd.read_pickle(self.data_dir / \"X_train.pkl\")\n",
    "        y_train = pd.read_pickle(self.data_dir / \"y_train.pkl\").squeeze()\n",
    "        return X_train, y_train\n",
    "\n",
    "    def determine_categorical_features(self, X_train: pd.DataFrame):\n",
    "        if self.cat_features:\n",
    "            cat_features = [col for col in self.cat_features if col in X_train.columns]\n",
    "        else:\n",
    "            cat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        for col in cat_features:\n",
    "            if col in X_train.columns:\n",
    "                # Ensure column is treated as category\n",
    "                X_train[col] = X_train[col].astype(\"category\")\n",
    "                \n",
    "                # Add \"missing\" only if it's not already a category\n",
    "                if \"missing\" not in X_train[col].cat.categories:\n",
    "                    X_train[col] = X_train[col].cat.add_categories(\"missing\")\n",
    "                \n",
    "                # Fill missing values with \"missing\"\n",
    "                X_train[col] = X_train[col].fillna(\"missing\")\n",
    "\n",
    "        self.logger.info(f\"Categorical features: {cat_features}\")\n",
    "        return cat_features\n",
    "\n",
    "\n",
    "    def cross_validate_model(self, X_train: pd.DataFrame, y_train: pd.Series, cat_features: list, cv: int = 5):\n",
    "        if self.model_name == 'catboost':\n",
    "            model = CatBoostClassifier(\n",
    "                random_seed=42, verbose=0, eval_metric='F1',\n",
    "                cat_features=cat_features, class_weights=[1, 10]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "\n",
    "        self.logger.info(f\"Performing {cv}-fold cross-validation...\")\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        fold_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "            self.logger.info(f\"Processing fold {fold + 1}...\")\n",
    "\n",
    "            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            self.logger.info(f\"Validation set shape for fold {fold + 1}: {X_fold_val.shape}\")\n",
    "\n",
    "            model.fit(X_fold_train, y_fold_train, eval_set=(X_fold_val, y_fold_val), use_best_model=True)\n",
    "\n",
    "            fold_score = model.best_score_['validation']['F1']\n",
    "            fold_scores.append(fold_score)\n",
    "\n",
    "            self.logger.info(f\"Fold {fold + 1} F1 score: {fold_score}\")\n",
    "\n",
    "        mean_cv_score = sum(fold_scores) / len(fold_scores)\n",
    "        self.logger.info(f\"Mean cross-validation F1 score: {mean_cv_score}\")\n",
    "        return mean_cv_score\n",
    "\n",
    "    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series, cat_features: list, val_size: float = 0.2):\n",
    "        if self.model_name == 'catboost':\n",
    "            model = CatBoostClassifier(\n",
    "            random_seed=42,\n",
    "            verbose=100,\n",
    "            eval_metric='F1',\n",
    "            cat_features=cat_features,\n",
    "           #auto_class_weights='Balanced',\n",
    "            max_depth=5,\n",
    "           #colsample_bylevel=0.7,\n",
    "            class_weights=[1, 1/a],\n",
    "            bagging_temperature=0.4,\n",
    "            grow_policy='SymmetricTree',\n",
    "          # one_hot_max_size = 40,\n",
    "          # learning_rate=0.1,\n",
    "         #  subsample=.67,   #lower subsample showed progress\n",
    "         #  max_leaves= 64, #only with lossguide\n",
    "            bootstrap_type = \"Bayesian\", #Bayesian uses the posterior probability of the object \n",
    "                                        #to sample the trees in the growing process. Good for regularization and overfitting control.\n",
    "          # bootstrap_type='Bernoulli', #Bernoulli is Stochastic Gradient Boosting on random subsets of features, faster and less overfitting\n",
    "            early_stopping_rounds=100,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "\n",
    "        self.logger.info(f\"Training {self.model_name} model...\")\n",
    "        X_train.drop(columns=['session_id', 'DateTime', 'user_id'], inplace=True, errors='ignore')\n",
    "\n",
    "        X_train_final, X_valid, y_train_final, y_valid = train_test_split(\n",
    "            X_train, y_train, test_size=val_size, random_state=42)\n",
    "        \n",
    "\n",
    "        self.logger.info(f\"Training {self.model_name} model with validation set...\")\n",
    "        model.fit(X_train_final, y_train_final, eval_set=(X_valid, y_valid), use_best_model=True)\n",
    "\n",
    "        return model\n",
    "\n",
    "# Interactive Workflow for Jupyter Notebook\n",
    "# Define the data directory\n",
    "DATA_DIR = \"path/to/data\"  # Replace with your actual path\n",
    "\n",
    "# Initialize ModelTrainer\n",
    "trainer = ModelTrainer(DATA_DIR,model_name=\"catboost\")\n",
    "\n",
    "# Load data\n",
    "#X_train, y_train = trainer.load_data()\n",
    "\n",
    "# Determine categorical features\n",
    "cat_features = trainer.determine_categorical_features(X_train)\n",
    "trainer.determine_categorical_features(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>webpage_id</th>\n",
       "      <th>user_group_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_level</th>\n",
       "      <th>user_depth</th>\n",
       "      <th>city_development_index</th>\n",
       "      <th>var_1</th>\n",
       "      <th>product_category</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>C</td>\n",
       "      <td>359520</td>\n",
       "      <td>13787</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197581</th>\n",
       "      <td>C</td>\n",
       "      <td>405490</td>\n",
       "      <td>60305</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43920</th>\n",
       "      <td>H</td>\n",
       "      <td>105960</td>\n",
       "      <td>11085</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332522</th>\n",
       "      <td>I</td>\n",
       "      <td>118601</td>\n",
       "      <td>28529</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45995</th>\n",
       "      <td>C</td>\n",
       "      <td>359520</td>\n",
       "      <td>13787</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product campaign_id webpage_id user_group_id gender  age_level  \\\n",
       "196          C      359520      13787             4   Male        4.0   \n",
       "197581       C      405490      60305             1   Male        1.0   \n",
       "43920        H      105960      11085             2   Male        2.0   \n",
       "332522       I      118601      28529             3   Male        3.0   \n",
       "45995        C      359520      13787             1   Male        1.0   \n",
       "\n",
       "        user_depth  city_development_index  var_1 product_category  hour  day  \\\n",
       "196            3.0                     NaN    0.0                4  11.0  4.0   \n",
       "197581         3.0                     2.0    0.0                3   7.0  6.0   \n",
       "43920          3.0                     4.0    0.0                5  13.0  4.0   \n",
       "332522         3.0                     2.0    1.0                4  22.0  7.0   \n",
       "45995          3.0                     3.0    0.0                4  12.0  2.0   \n",
       "\n",
       "        month  \n",
       "196       7.0  \n",
       "197581    7.0  \n",
       "43920     7.0  \n",
       "332522    7.0  \n",
       "45995     7.0  "
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training catboost model...\n",
      "INFO:__main__:Training catboost model with validation set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.121943\n",
      "0:\tlearn: 0.5522078\ttest: 0.5594013\tbest: 0.5594013 (0)\ttotal: 260ms\tremaining: 4m 19s\n",
      "100:\tlearn: 0.5980550\ttest: 0.5837107\tbest: 0.5853416 (38)\ttotal: 49.2s\tremaining: 7m 18s\n",
      "200:\tlearn: 0.6214058\ttest: 0.5819342\tbest: 0.5888700 (113)\ttotal: 1m 49s\tremaining: 7m 15s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.5888699592\n",
      "bestIteration = 113\n",
      "\n",
      "Shrink model to first 114 iterations.\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train_model(X_train, y_train, cat_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.5203    0.6717     68166\n",
      "         1.0     0.0847    0.6059    0.1486      4994\n",
      "\n",
      "    accuracy                         0.5262     73160\n",
      "   macro avg     0.5161    0.5631    0.4102     73160\n",
      "weighted avg     0.8885    0.5262    0.6360     73160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.54\n",
      "Best F1 score: 0.1525829895894716\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# After training your CatBoost model:\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "# We can search thresholds from 0.0 to 1.0 in small steps\n",
    "for t in np.linspace(0, 1, 101):\n",
    "    y_pred = (y_probs >= t).astype(int)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Best F1 score: {best_f1}\")\n",
    "\n",
    "# When predicting on test data, use the best_threshold:\n",
    "y_probs_test = model.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = (y_probs_test >= best_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive preprocessing (not my class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= raw_data.drop(columns = ['session_id', 'DateTime', 'user_id','product_category_2'])\n",
    "data2.dropna(inplace = True) # Just close your eyes and drop the rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = data2.drop(columns = ['is_click'])\n",
    "y_2 = data2['is_click']\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_d2 = [\"product\", \"campaign_id\", \"webpage_id\", \"user_group_id\", \"gender\", \"product_category_1\"]\n",
    "\n",
    "X_train_2_d = pd.get_dummies(X_train_2, columns = columns_to_d2)\n",
    "X_test_2_d = pd.get_dummies(X_test_2, columns = columns_to_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42,class_weight='balanced')\n",
    "rf.fit(X_train_2_d, y_train_2)\n",
    "y_pred_RF = rf.predict(X_test_2_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14298031865042174"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_2, y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14430736693690518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train_2_d, y_train_2)\n",
    "\n",
    "y_pred_LR = model.predict(X_test_2_d)\n",
    "print(f1_score(y_test_2, y_pred_LR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yofi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
