{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Univariate feature imputation - SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0) that contain the missing values:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "SimpleImputer()\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))\n",
    "[[4.          2.        ]\n",
    " [6.          3.666...]\n",
    " [7.          6.        ]]\n",
    "```\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                   [np.nan, \"y\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype=\"category\")\n",
    "\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "print(imp.fit_transform(df))\n",
    "[['a' 'x']\n",
    " ['a' 'y']\n",
    " ['a' 'y']\n",
    " ['b' 'y']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate feature imputation - IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "IterativeImputer(random_state=0)\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "print(np.round(imp.transform(X_test)))\n",
    "[[ 1.  2.]\n",
    " [ 6. 12.]\n",
    " [ 3.  6.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the statistics community, it is common practice to perform multiple imputations, generating, for example, m separate imputations for a single feature matrix. Each of these m imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The m final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation.\n",
    "\n",
    "Our implementation of IterativeImputer was inspired by the R MICE package (Multivariate Imputation by Chained Equations) [1], but differs from it by returning a single imputation instead of multiple imputations. However, IterativeImputer can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when sample_posterior=True. See [2], chapter 4 for more discussion on multiple vs. single imputations.\n",
    "\n",
    "It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values.\n",
    "\n",
    "Note that a call to the transform method of IterativeImputer is not allowed to change the number of samples. Therefore multiple imputations cannot be achieved by a single call to transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbors imputation- KNNImputer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform. For more information on the methodology, see ref. [OL2001]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer.fit_transform(X)\n",
    "array([[1. , 2. , 4. ],\n",
    "       [3. , 4. , 3. ],\n",
    "       [5.5, 6. , 5. ],\n",
    "       [8. , 8. , 7. ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Sample dataset with missing values in user_group_id, age_level, gender\n",
    "data = {\n",
    "    \"user_group_id\": [1, 2, np.nan, 2, 1, np.nan, 3, np.nan, 4, 2],\n",
    "    \"age_level\": [3, 4, 3, np.nan, 2, np.nan, 3, 5, np.nan, 1],\n",
    "    \"gender\": [\"Male\", \"Female\", np.nan, \"Female\", \"Male\", \"Female\", \"Male\", np.nan, \"Female\", \"Male\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = [\"user_group_id\", \"gender\"]\n",
    "numerical_cols = [\"age_level\"]\n",
    "\n",
    "# Encode categorical variables using Ordinal Encoding\n",
    "encoder = OrdinalEncoder()\n",
    "df[categorical_cols] = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Define function to choose the right estimator\n",
    "def get_imputer(strategy=\"regressor\"):\n",
    "    if strategy == \"classifier\":\n",
    "        return RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    else:\n",
    "        return RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 1: Impute categorical variables first (user_group_id, gender)\n",
    "iterative_imputer_categorical = IterativeImputer(estimator=get_imputer(\"classifier\"), max_iter=10, random_state=42)\n",
    "df[categorical_cols] = iterative_imputer_categorical.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Step 2: Impute numerical variable (age_level) using the now-filled categorical variables\n",
    "iterative_imputer_numeric = IterativeImputer(estimator=get_imputer(\"regressor\"), max_iter=10, random_state=42)\n",
    "df[numerical_cols] = iterative_imputer_numeric.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Convert categorical columns back to original labels\n",
    "df[categorical_cols] = encoder.inverse_transform(df[categorical_cols])\n",
    "\n",
    "# Print final imputed dataset\n",
    "print(\"Imputed Data:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import lightgbm as lgb  # Using LightGBM for boosting-based imputation\n",
    "\n",
    "# Sample dataset with missing values\n",
    "data = {\n",
    "    \"user_group_id\": [1, 2, np.nan, 2, 1, np.nan, 3, np.nan, 4, 2],\n",
    "    \"age_level\": [3, 4, 3, np.nan, 2, np.nan, 3, 5, np.nan, 1],\n",
    "    \"gender\": [\"Male\", \"Female\", np.nan, \"Female\", \"Male\", \"Female\", \"Male\", np.nan, \"Female\", \"Male\"],\n",
    "    \"campaign_id\": [405490, 118601, np.nan, 359520, 405490, np.nan, 359520, 118601, np.nan, 405490],\n",
    "    \"webpage_id\": [60305, 28529, 13787, np.nan, 60305, np.nan, 13787, 28529, 13787, 60305]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = [\"user_group_id\", \"gender\", \"campaign_id\", \"webpage_id\"]\n",
    "numerical_cols = [\"age_level\"]\n",
    "\n",
    "# Encode categorical variables using Ordinal Encoding (needed for LightGBM)\n",
    "encoder = OrdinalEncoder()\n",
    "df[categorical_cols] = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Define function to create LightGBM estimators\n",
    "def get_boosting_imputer(strategy=\"regressor\"):\n",
    "    if strategy == \"classifier\":\n",
    "        return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "    else:\n",
    "        return lgb.LGBMRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Step 1: Impute categorical features first using LightGBMClassifier\n",
    "iterative_imputer_categorical = IterativeImputer(estimator=get_boosting_imputer(\"classifier\"), max_iter=10, random_state=42)\n",
    "df[categorical_cols] = iterative_imputer_categorical.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Step 2: Impute numerical feature (age_level) using LightGBMRegressor\n",
    "iterative_imputer_numeric = IterativeImputer(estimator=get_boosting_imputer(\"regressor\"), max_iter=10, random_state=42)\n",
    "df[numerical_cols] = iterative_imputer_numeric.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Convert categorical columns back to original labels\n",
    "df[categorical_cols] = encoder.inverse_transform(df[categorical_cols])\n",
    "\n",
    "# Print final imputed dataset\n",
    "print(\"Imputed Data:\")\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yofi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
